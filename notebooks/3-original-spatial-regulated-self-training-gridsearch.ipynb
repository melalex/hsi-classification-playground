{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search hyper parameters for SRST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "from torch import nn\n",
    "from typing import Sequence\n",
    "\n",
    "from src.model.grid_search import GridSearchAdapter\n",
    "from src.util.torch_device import resolve_torch_device\n",
    "from src.data.indian_pines import load_indian_pines\n",
    "from src.pipeline.spatial_regulated_self_training_pipeline import (\n",
    "    SpatialRegulatedSelfTrainingPipeline,\n",
    "    SpatialRegulatedSelfTrainingPipelineArgs,\n",
    "    KMeansClustering,\n",
    "    ClassificationFeatureExtractor,\n",
    ")\n",
    "from src.trainer.classification_trainer import ClassificationTrainer\n",
    "from src.model.fully_convolutional_lenet import FullyConvolutionalLeNet\n",
    "from src.util.semi_guided import sample_from_segmentation_matrix\n",
    "from src.util.over_clustering import exponential_decay_over_clustering\n",
    "from src.definitions import GREED_SEARCH_FOLDER\n",
    "from src.model.grid_search import GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prepare env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "device = resolve_torch_device()\n",
    "\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Device is {device}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indian pines (Cluster exponential decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_class = 15\n",
    "epoch_seconds = int(time.time())\n",
    "run_name = f\"indian-pines-cluster-exponential-decay-{epoch_seconds}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "f\"Setting num_workers to {cpu_count}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, labels = load_indian_pines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(labels))\n",
    "\n",
    "f\"Number of classes {num_classes}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_labels = sample_from_segmentation_matrix(labels, examples_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialRegulatedSelfTrainingPipelineGridSearchAdapter(\n",
    "    GridSearchAdapter[SpatialRegulatedSelfTrainingPipeline]\n",
    "):\n",
    "\n",
    "    def params_grid(self) -> dict[str, Sequence[float]]:\n",
    "        return {\n",
    "            \"splits\": [4],\n",
    "            \"learning_rate\": [1e-3, 1e-4, 1e-5],\n",
    "            \"patch_size\": [9],\n",
    "            \"num_epochs\": [11],\n",
    "            \"feature_extractor_epochs\": [1, 5, 9, 11],\n",
    "            \"semantic_threshold\": [0.5, 0.6, 0.7, 0.8],\n",
    "            \"lambda_v\": [0.07, 0.09, 0.2, 0.3, 0.4, 0.49],\n",
    "            \"k_star\": [num_classes * 2, num_classes * 3],\n",
    "            \"batch_size\": [64]\n",
    "        }\n",
    "\n",
    "    def init_model(self, params: dict[str, float]):\n",
    "        _, _, c = image.shape\n",
    "\n",
    "        input_channels = int(c / params[\"splits\"])\n",
    "\n",
    "        model = FullyConvolutionalLeNet(input_channels, num_classes).to(device)\n",
    "\n",
    "        trainer = ClassificationTrainer(\n",
    "            num_epochs=params[\"feature_extractor_epochs\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            loss_fun=nn.CrossEntropyLoss(),\n",
    "        )\n",
    "\n",
    "        k_values = exponential_decay_over_clustering(\n",
    "            k_star=params[\"k_star\"],\n",
    "            lambda_v=params[\"lambda_v\"],\n",
    "            max_iter=params[\"num_epochs\"],\n",
    "        )\n",
    "\n",
    "        args = SpatialRegulatedSelfTrainingPipelineArgs(\n",
    "            num_classes=num_classes,\n",
    "            cluster_sizes=k_values,\n",
    "            feature_extractor=ClassificationFeatureExtractor(\n",
    "                model, trainer, generator, batch_size=params[\"batch_size\"]\n",
    "            ),\n",
    "            clustering=KMeansClustering(seed=random_seed),\n",
    "            splits=params[\"splits\"],\n",
    "            patch_size=params[\"patch_size\"],\n",
    "            init_patch_size=5,\n",
    "            semantic_threshold=params[\"semantic_threshold\"],\n",
    "            spatial_threshold=8,\n",
    "            spatial_constraint_weights=[1, 0.5],\n",
    "            record_step_snapshots=True,\n",
    "        )\n",
    "\n",
    "        return SpatialRegulatedSelfTrainingPipeline(args, device)\n",
    "\n",
    "    def fit_model(self, model: SpatialRegulatedSelfTrainingPipeline):\n",
    "        model.fit(image, masked_labels, labels)\n",
    "\n",
    "    def score_model(\n",
    "        self, model: SpatialRegulatedSelfTrainingPipeline\n",
    "    ) -> list[dict[str, float]]:\n",
    "        return [it.metrics.__dict__ for it in model.history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = GREED_SEARCH_FOLDER / f\"{run_name}.csv\"\n",
    "\n",
    "search = GridSearch(\n",
    "    adapter=SpatialRegulatedSelfTrainingPipelineGridSearchAdapter(),\n",
    "    optimize_metric=\"kappa_score\",\n",
    "    log_file=log_file,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, best_params, best_score = search.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Params:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pd.read_csv(log_file)\n",
    "\n",
    "report.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
