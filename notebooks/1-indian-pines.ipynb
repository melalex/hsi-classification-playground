{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indiana Pines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "import torch.utils.data as data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from lightning import Trainer\n",
    "\n",
    "from src.util.torch_device import resolve_torch_device\n",
    "from src.util.patches import extract_patches, scale_patched\n",
    "from src.data.indian_pines import load_indian_pines\n",
    "from src.model.hsic import HyperSpectralImageClassifier\n",
    "from src.model.fully_convolutional_lenet import FullyConvolutionalLeNet\n",
    "from src.visualization.plot import plot_segmentation_comparison\n",
    "from src.model.hsi_vit import HsiVisionTransformer, Conv3DStem\n",
    "from src.data.dataset_decorator import UnlabeledDatasetDecorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Prepare env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "batch_size = 32\n",
    "patch_size = 9\n",
    "num_epochs = 12\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = resolve_torch_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Device is mps'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Device is {device}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, labels = load_indian_pines()\n",
    "\n",
    "image_h, image_w, image_c = image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = extract_patches(image, labels, patch_size=patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale, x = scale_patched(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=random_seed, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.tensor(x, dtype=torch.float32).permute(0, 3, 1, 2) \n",
    "y_tensor = torch.tensor(x, dtype=torch.long)\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Setting num_workers to 14'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_count = multiprocessing.cpu_count()\n",
    "\n",
    "f\"Setting num_workers to {cpu_count}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = data.TensorDataset(x_test_tensor, y_test_tensor)\n",
    "full_dataset = data.TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cpu_count,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "test_loader = data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=cpu_count,\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training samples: 16820, Testing samples: 4205'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Training samples: {len(train_dataset)}, Testing samples: {len(test_dataset)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Number of classes 17'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "\n",
    "f\"Number of classes {num_classes}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HyperSpectralImageClassifier(\n",
    "    FullyConvolutionalLeNet(image_c, num_classes), num_classes, lr=learning_rate\n",
    ")\n",
    "\n",
    "trainer = Trainer(accelerator=\"auto\", devices=1, max_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(x_tensor)\n",
    "y_pred = torch.argmax(y_pred, dim=1)\n",
    "y_pred = y_pred.reshape(image_h, image_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_segmentation_comparison(y.reshape(image_h, image_w), y_pred.numpy(), num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size=16\n",
    "num_layers=6\n",
    "num_heads=6\n",
    "hidden_dim=384\n",
    "mlp_dim=1536\n",
    "learning_rate=1e-2\n",
    "num_epochs=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/alexandermelashchenko/Workspace/spatial-regulated-self-training/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "model = HyperSpectralImageClassifier(\n",
    "    HsiVisionTransformer(\n",
    "        patch_size=patch_size,\n",
    "        num_layers=num_layers,\n",
    "        num_heads=num_heads,\n",
    "        hidden_dim=hidden_dim,\n",
    "        mlp_dim=mlp_dim,\n",
    "    ),\n",
    "    num_classes,\n",
    "    lr=learning_rate,\n",
    ")\n",
    "\n",
    "trainer = Trainer(accelerator=\"auto\", devices=1, max_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name             | Type                 | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | net              | HsiVisionTransformer | 12.0 M | train\n",
      "1 | f1               | MulticlassF1Score    | 0      | train\n",
      "2 | overall_accuracy | MulticlassAccuracy   | 0      | train\n",
      "3 | average_accuracy | MulticlassAccuracy   | 0      | train\n",
      "4 | kappa            | MulticlassCohenKappa | 0      | train\n",
      "------------------------------------------------------------------\n",
      "12.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.0 M    Total params\n",
      "47.948    Total estimated model params size (MB)\n",
      "98        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830d168f8b454e77a20958c3504f648a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fb4d4bf9ac4e62896bffc9f032275a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6317c252130a4f4eb6af8fae23e5da97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecee132ec0e486a96f8ddaead19a2a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45dc929020e14e3a9196edc68cb632bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Display prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandermelashchenko/Workspace/spatial-regulated-self-training/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:476: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c741d8e3e4994476b8540a6bc443247a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_loader = data.DataLoader(\n",
    "    UnlabeledDatasetDecorator(full_dataset),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cpu_count,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "y_pred = trainer.predict(model, predict_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.cat(y_pred, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = torch.argmax(y_pred, dim=1)\n",
    "y_pred = y_pred.reshape(image_h, image_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_segmentation_comparison(y.reshape(image_h, image_w), y_pred.numpy(), num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
